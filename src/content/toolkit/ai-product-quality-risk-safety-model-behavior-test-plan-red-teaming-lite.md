---
title: "Model behavior test plan (red teaming-lite)"
primaryCategory: "AI Product Quality, Risk & Safety"
categories: ["AI Product Quality, Risk & Safety"]
whenToUse: "When you suspect the model may hallucinate, jailbreak, or produce unsafe outputs—and you need to test it before launch."
inputsRequired: "Abuse prompts; adversarial scenarios; policy rules; golden set; known failure modes."
outputArtifact: "Red-team suite; results; fixes; re-test cadence."
commonMistakes: "Only testing normal users; no jailbreak attempts; no retest after fixes."
dbTitle: "Frameworks"
notionId: "2db39950-eddd-8007-ac36-fc38e85fbb37"
link: ""
cover: "/toolkit-covers/ai-product-quality-risk-safety-model-behavior-test-plan-red-teaming-lite.png"
files: []
---
## When to use
When you suspect the model may hallucinate, jailbreak, or produce unsafe outputs—and you need to test it before launch.

## Inputs required
Abuse prompts; adversarial scenarios; policy rules; golden set; known failure modes.

## Output artifact
Red-team suite; results; fixes; re-test cadence.

## Common mistakes
Only testing normal users; no jailbreak attempts; no retest after fixes.
