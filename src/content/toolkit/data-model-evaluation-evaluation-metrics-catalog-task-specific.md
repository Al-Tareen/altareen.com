---
title: "Evaluation Metrics Catalog (Task-specific)"
primaryCategory: "Data & Model Evaluation"
categories: ["Data & Model Evaluation"]
whenToUse: "When you must choose a model/prompt approach and need a repeatable, defensible way to compare options."
inputsRequired: "Golden set; metrics definitions; baseline approach; acceptance thresholds; test harness."
outputArtifact: "Eval plan; scorecards; ship/no-ship thresholds; comparison results."
commonMistakes: "No thresholds; metrics don’t match user value; offline-only; no baseline comparison."
dbTitle: "Frameworks"
notionId: "2db39950-eddd-8033-84f9-f1f69ba87f5d"
link: ""
cover: "/toolkit-covers/data-model-evaluation-evaluation-metrics-catalog-task-specific.png"
files: []
---
## When to use
When you must choose a model/prompt approach and need a repeatable, defensible way to compare options.

## Inputs required
Golden set; metrics definitions; baseline approach; acceptance thresholds; test harness.

## Output artifact
Eval plan; scorecards; ship/no-ship thresholds; comparison results.

## Common mistakes
No thresholds; metrics don’t match user value; offline-only; no baseline comparison.
