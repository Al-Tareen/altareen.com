---
title: "Guardrails & escalation policy (what happens when model fails)"
primaryCategory: "AI Product Quality, Risk & Safety"
categories: ["AI Product Quality, Risk & Safety"]
whenToUse: "When failures would harm trust and you need clear behavior for refusal, fallback, and escalation."
inputsRequired: "Failure modes; confidence/quality thresholds; fallback options; escalation owners; logging."
outputArtifact: "Guardrail rules (refuse/fallback/hand-off); escalation map; severity levels."
commonMistakes: "Over-restricting (kills UX) or under-restricting (unsafe); vague thresholds; no fallback."
dbTitle: "Frameworks"
notionId: "2db39950-eddd-8040-aa9c-f396e91ee507"
link: ""
cover: "/toolkit-covers/ai-product-quality-risk-safety-guardrails-escalation-policy-what-happens-when-model-fails.png"
files: []
---
## When to use
When failures would harm trust and you need clear behavior for refusal, fallback, and escalation.

## Inputs required
Failure modes; confidence/quality thresholds; fallback options; escalation owners; logging.

## Output artifact
Guardrail rules (refuse/fallback/hand-off); escalation map; severity levels.

## Common mistakes
Over-restricting (kills UX) or under-restricting (unsafe); vague thresholds; no fallback.
