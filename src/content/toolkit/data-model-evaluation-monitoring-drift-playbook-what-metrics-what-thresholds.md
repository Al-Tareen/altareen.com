---
title: "Monitoring & drift playbook (what metrics, what thresholds)"
primaryCategory: "Data & Model Evaluation"
categories: ["Data & Model Evaluation"]
whenToUse: "When an AI feature is live (or about to go live) and you need to detect quality decay, drift, or abuse fast."
inputsRequired: "Instrumentation events; quality signals; thresholds; on-call ownership; dashboards tools."
outputArtifact: "Monitoring plan; alert rules; dashboards; runbooks for common failures."
commonMistakes: "Monitoring uptime only; no quality monitoring; alert fatigue; unclear ownership."
dbTitle: "Frameworks"
notionId: "2db39950-eddd-80f2-9f69-ec61c5fd5865"
link: ""
cover: "/toolkit-covers/data-model-evaluation-monitoring-drift-playbook-what-metrics-what-thresholds.png"
files: []
---
## When to use
When an AI feature is live (or about to go live) and you need to detect quality decay, drift, or abuse fast.

## Inputs required
Instrumentation events; quality signals; thresholds; on-call ownership; dashboards tools.

## Output artifact
Monitoring plan; alert rules; dashboards; runbooks for common failures.

## Common mistakes
Monitoring uptime only; no quality monitoring; alert fatigue; unclear ownership.
